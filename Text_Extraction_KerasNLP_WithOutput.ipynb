{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [KerasNLP] question answering by span labeling with BERT\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv), updated by [Usha Rengaraju](https://www.linkedin.com/in/usha-rengaraju-b570b7a2/)<br>\n",
        "**Date created:** 2023/06/21<br>\n",
        "**Last modified:** 2023/06/21<br>\n",
        "**Description:** Fine tune pretrained BERT from KerasNLP on SQuAD."
      ],
      "metadata": {
        "id": "1zzKPOkZ7TOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "The notebook demonstrates how to find the span of text in the paragraph that answers the question using KerasNLP . KerasNLP is highly modular library for natural language processing with state-of-the-art preset weights and out-of-the-box architectures which can be customized when needed.\n",
        "\n",
        "In this example ,We fine-tune a BERT model to perform text extraction as follows:\n",
        "\n",
        "1. Feed the context and the question as inputs to BERT.\n",
        "2. Take two vectors S and T with dimensions equal to that of\n",
        "   hidden states in BERT.\n",
        "3. Compute the probability of each token being the start and end of\n",
        "   the answer span. The probability of a token being the start of\n",
        "   the answer is given by a dot product between S and the representation\n",
        "   of the token in the last layer of BERT, followed by a softmax over all tokens.\n",
        "   The probability of a token being the end of the answer is computed\n",
        "   similarly with the vector T.\n",
        "4. Fine-tune BERT and learn S and T along the way."
      ],
      "metadata": {
        "id": "Kk2-e1tD7Y5s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2zB_9o5cBAr",
        "outputId": "a786ab87-9097-4352-e097-06939c757279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.7/527.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q keras-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-aMpdhacDS1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up BERT tokenizer\n",
        "BertTokenizer from KerasNLP is used to convert strings to tf.RaggedTensors of token ids."
      ],
      "metadata": {
        "id": "a6yF5B2l7uxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as tf_text\n",
        "max_len=384\n",
        "tok = keras_nlp.models.BertTokenizer.from_preset(\"bert_base_en_uncased\", lowercase=True)\n",
        "tokenizer = tf_text.FastWordpieceTokenizer(tok.vocabulary,support_detokenization=True)"
      ],
      "metadata": {
        "id": "FG9WcD5DFpED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data\n",
        "\n",
        "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset,consisting of questions from Wikipedia articles, where the answer is a segment of text, or span, from the reading passage.The SQuAD dataset is loaded using keras.utils.text_dataset_from_directory, which utilizes the tf.data.Dataset format."
      ],
      "metadata": {
        "id": "zA4ym4h47p5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xNYxkJQfiOJ",
        "outputId": "aaa02cd1-cf45-4e8d-9371-93ade7096bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "30288272/30288272 [==============================] - 0s 0us/step\n",
            "Downloading data from https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "4854279/4854279 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "train_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\"\n",
        "train_path = keras.utils.get_file(\"train.json\", train_data_url)\n",
        "eval_data_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
        "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_value=tok.cls_token_id,\n",
        "end_value=tok.sep_token_id,\n",
        "packer = keras_nlp.layers.MultiSegmentPacker(max_len, start_value, end_value)"
      ],
      "metadata": {
        "id": "gZOFgkMCQYY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data\n",
        "\n",
        "1. Go through the JSON file and store every record as a `SquadExample` object.\n",
        "2. Go through each `SquadExample` and create `x_train, y_train, x_eval, y_eval`."
      ],
      "metadata": {
        "id": "YCl6quOa758X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "class SquadExample:\n",
        "    def __init__(self, question, context, start_char_idx, answer_text, all_answers):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.skip = False\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = self.context\n",
        "        question = self.question\n",
        "        answer_text = self.answer_text\n",
        "        start_char_idx = self.start_char_idx\n",
        "\n",
        "        context = \" \".join(str(context).split())\n",
        "        question = \" \".join(str(question).split())\n",
        "        answer = \" \".join(str(answer_text).split())\n",
        "\n",
        "        end_char_idx = start_char_idx + len(answer)\n",
        "        if end_char_idx >= len(context):\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        is_char_in_ans = [0] * len(context)\n",
        "        for idx in range(start_char_idx, end_char_idx):\n",
        "            is_char_in_ans[idx] = 1\n",
        "\n",
        "        tokenized_context = tokenizer.tokenize_with_offsets(context)\n",
        "\n",
        "        ans_token_idx = []\n",
        "        for idx,(_,start,end) in enumerate(zip(tokenized_context[0],tokenized_context[1],tokenized_context[2])):\n",
        "            if sum(is_char_in_ans[start:end]) > 0:\n",
        "                ans_token_idx.append(idx)\n",
        "\n",
        "        if len(ans_token_idx) == 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "\n",
        "        start_token_idx = ans_token_idx[0]\n",
        "        end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "        tokenized_question = tokenizer.tokenize_with_offsets(question)\n",
        "\n",
        "        packed = packer((tokenized_context[0],tokenized_question[0][1:]))\n",
        "        input_ids = packed[0]\n",
        "        token_type_ids = packed[1]\n",
        "        padding_mask = input_ids!=0\n",
        "\n",
        "\n",
        "        self.input_ids = input_ids\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.padding_mask = padding_mask\n",
        "        self.start_token_idx = start_token_idx\n",
        "        self.end_token_idx = end_token_idx\n",
        "        self.context_token_to_char = tuple(zip(tokenized_context[1],tokenized_context[2]))\n",
        "\n",
        "\n",
        "with open(train_path) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(eval_path) as f:\n",
        "    raw_eval_data = json.load(f)\n",
        "\n",
        "\n",
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in tqdm(raw_data[\"data\"][:1]):\n",
        "        for para in item[\"paragraphs\"][:3]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                squad_eg = SquadExample(\n",
        "                    question, context, start_char_idx, answer_text, all_answers\n",
        "                )\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"token_type_ids\": [],\n",
        "        \"padding_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"token_type_ids\"],\n",
        "        dataset_dict[\"padding_mask\"],\n",
        "    ]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_squad_examples = create_squad_examples(raw_train_data)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")\n",
        "\n",
        "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
      ],
      "metadata": {
        "id": "YsMfGaYQJEtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3eb0a1e-44e4-48bd-f2f8-fa962c8e700e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 training points created.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:26<00:00, 26.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80 evaluation points created.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Question-Answering Model using BertBackbone from KerasNLP which distills the input tokens into dense features that can be used in downstream tasks."
      ],
      "metadata": {
        "id": "sz4AhoUn8DRV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_o2KgyBnfoM"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    ## BERT encoder\n",
        "    encoder = keras_nlp.models.BertBackbone.from_preset(\"bert_base_en_uncased\")\n",
        "\n",
        "    ## QA Model\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding = encoder(\n",
        "        [input_ids, token_type_ids, attention_mask]\n",
        "    )['sequence_output']\n",
        "\n",
        "    start_logits = layers.Dense(1, name=\"start_logit\",kernel_initializer='he_uniform')(embedding)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, name=\"end_logit\",kernel_initializer='he_uniform')(embedding)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "    model = keras.Model(\n",
        "        inputs=[input_ids, token_type_ids, attention_mask],\n",
        "        outputs=[start_probs, end_probs],\n",
        "    )\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = keras.optimizers.Adam(10e-7,clipnorm=1.0)\n",
        "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOWD-wmDFs2i",
        "outputId": "7c1633c8-e005-4026-ae21-c2dcc5b41585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_34 (InputLayer)       [(None, 384)]                0         []                            \n",
            "                                                                                                  \n",
            " input_35 (InputLayer)       [(None, 384)]                0         []                            \n",
            "                                                                                                  \n",
            " input_36 (InputLayer)       [(None, 384)]                0         []                            \n",
            "                                                                                                  \n",
            " bert_backbone_11 (BertBack  {'sequence_output': (None,   1094822   ['input_34[0][0]',            \n",
            " bone)                        None, 768),                 40         'input_35[0][0]',            \n",
            "                              'pooled_output': (None, 7              'input_36[0][0]']            \n",
            "                             68)}                                                                 \n",
            "                                                                                                  \n",
            " start_logit (Dense)         (None, 384, 1)               769       ['bert_backbone_11[0][1]']    \n",
            "                                                                                                  \n",
            " end_logit (Dense)           (None, 384, 1)               769       ['bert_backbone_11[0][1]']    \n",
            "                                                                                                  \n",
            " flatten_22 (Flatten)        (None, 384)                  0         ['start_logit[0][0]']         \n",
            "                                                                                                  \n",
            " flatten_23 (Flatten)        (None, 384)                  0         ['end_logit[0][0]']           \n",
            "                                                                                                  \n",
            " activation_20 (Activation)  (None, 384)                  0         ['flatten_22[0][0]']          \n",
            "                                                                                                  \n",
            " activation_21 (Activation)  (None, 384)                  0         ['flatten_23[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109483778 (417.65 MB)\n",
            "Trainable params: 109483778 (417.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to the TPU"
      ],
      "metadata": {
        "id": "Iw9Oo9j08KTx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LYcxGKWRQjE"
      },
      "outputs": [],
      "source": [
        "try: # detect TPUs\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError: # detect GPUs\n",
        "    strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
        "\n",
        "# Create model\n",
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create evaluation Callback\n",
        "\n",
        "This callback will compute the exact match score using the validation data\n",
        "after every epoch."
      ],
      "metadata": {
        "id": "cVdOOhoP8SHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    exclude = set(string.punctuation)\n",
        "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    # Remove articles\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # Remove extra white space\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "\n",
        "class ExactMatch(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        count = 0\n",
        "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            squad_eg = eval_examples_no_skip[idx]\n",
        "            offsets = squad_eg.context_token_to_char\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if start >= len(offsets):\n",
        "                continue\n",
        "            pred_char_start = offsets[start][0]\n",
        "            if end < len(offsets):\n",
        "                pred_char_end = offsets[end][1]\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\n",
        "\n",
        "            normalized_pred_ans = normalize_text(pred_ans)\n",
        "            normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                count += 1\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nepoch={epoch+1}, exact match score={acc:.2f}\")"
      ],
      "metadata": {
        "id": "1VrQhUGTQ8Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate"
      ],
      "metadata": {
        "id": "PuC3m-vB8Ybl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=1,  # For demonstration, 3 epochs are recommended\n",
        "    verbose=1,\n",
        "    batch_size=2,\n",
        "    callbacks=[exact_match_callback],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP9jEaY7VbT9",
        "outputId": "06fa2a0a-f66a-4d80-d2a3-19c698f07f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['pooled_dense/kernel:0', 'pooled_dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['pooled_dense/kernel:0', 'pooled_dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['pooled_dense/kernel:0', 'pooled_dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['pooled_dense/kernel:0', 'pooled_dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 4s 660ms/step\n",
            "\n",
            "epoch=1, exact match score=0.00\n",
            "8/8 [==============================] - 63s 819ms/step - loss: 12.0577 - activation_20_loss: 5.9109 - activation_21_loss: 6.1469\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f75e8ee6410>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIQ1_o7xpj9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afb70731-88cc-498a-f2c5-0e86df68b2de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 2s 672ms/step\n"
          ]
        }
      ],
      "source": [
        "pred = model.predict(x_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxDzyi6OdXMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424c6af5-ca00-43fe-940e-43a91c2c6b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context:  [CLS] [UNK] [UNK] 50 was an [UNK] football game to determine the champion of the [UNK] [UNK] [UNK] ( [UNK] ) for the 2015 season . [UNK] [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] defeated the [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] 24 – 10 to earn their third [UNK] [UNK] title . [UNK] game was played on [UNK] 7 , 2016 , at [UNK] ' s [UNK] in the [UNK] [UNK] [UNK] [UNK] at [UNK] [UNK] , [UNK] . [UNK] this was the 50th [UNK] [UNK] , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspending the tradition of naming each [UNK] [UNK] game with [UNK] numerals ( under which the game would have been known as \" [UNK] [UNK] [UNK] \" ) , so that the logo could prominently feature the [UNK] numerals 50 . \n",
            "question:   [UNK] team represented the [UNK] at [UNK] [UNK] 50 ? \n",
            "answer:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "context:  [CLS] [UNK] [UNK] 50 was an [UNK] football game to determine the champion of the [UNK] [UNK] [UNK] ( [UNK] ) for the 2015 season . [UNK] [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] defeated the [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] 24 – 10 to earn their third [UNK] [UNK] title . [UNK] game was played on [UNK] 7 , 2016 , at [UNK] ' s [UNK] in the [UNK] [UNK] [UNK] [UNK] at [UNK] [UNK] , [UNK] . [UNK] this was the 50th [UNK] [UNK] , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspending the tradition of naming each [UNK] [UNK] game with [UNK] numerals ( under which the game would have been known as \" [UNK] [UNK] [UNK] \" ) , so that the logo could prominently feature the [UNK] numerals 50 . \n",
            "question:   [UNK] team represented the [UNK] at [UNK] [UNK] 50 ? \n",
            "answer:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "context:  [CLS] [UNK] [UNK] 50 was an [UNK] football game to determine the champion of the [UNK] [UNK] [UNK] ( [UNK] ) for the 2015 season . [UNK] [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] defeated the [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] 24 – 10 to earn their third [UNK] [UNK] title . [UNK] game was played on [UNK] 7 , 2016 , at [UNK] ' s [UNK] in the [UNK] [UNK] [UNK] [UNK] at [UNK] [UNK] , [UNK] . [UNK] this was the 50th [UNK] [UNK] , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspending the tradition of naming each [UNK] [UNK] game with [UNK] numerals ( under which the game would have been known as \" [UNK] [UNK] [UNK] \" ) , so that the logo could prominently feature the [UNK] numerals 50 . \n",
            "question:   did [UNK] [UNK] 50 take place ? \n",
            "answer:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "context:  [CLS] [UNK] [UNK] 50 was an [UNK] football game to determine the champion of the [UNK] [UNK] [UNK] ( [UNK] ) for the 2015 season . [UNK] [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] defeated the [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] 24 – 10 to earn their third [UNK] [UNK] title . [UNK] game was played on [UNK] 7 , 2016 , at [UNK] ' s [UNK] in the [UNK] [UNK] [UNK] [UNK] at [UNK] [UNK] , [UNK] . [UNK] this was the 50th [UNK] [UNK] , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspending the tradition of naming each [UNK] [UNK] game with [UNK] numerals ( under which the game would have been known as \" [UNK] [UNK] [UNK] \" ) , so that the logo could prominently feature the [UNK] numerals 50 . \n",
            "question:   [UNK] team won [UNK] [UNK] 50 ? \n",
            "answer:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "context:  [CLS] [UNK] [UNK] 50 was an [UNK] football game to determine the champion of the [UNK] [UNK] [UNK] ( [UNK] ) for the 2015 season . [UNK] [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] defeated the [UNK] [UNK] [UNK] ( [UNK] ) champion [UNK] [UNK] 24 – 10 to earn their third [UNK] [UNK] title . [UNK] game was played on [UNK] 7 , 2016 , at [UNK] ' s [UNK] in the [UNK] [UNK] [UNK] [UNK] at [UNK] [UNK] , [UNK] . [UNK] this was the 50th [UNK] [UNK] , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspending the tradition of naming each [UNK] [UNK] game with [UNK] numerals ( under which the game would have been known as \" [UNK] [UNK] [UNK] \" ) , so that the logo could prominently feature the [UNK] numerals 50 . \n",
            "question:   color was used to emphasize the 50th anniversary of the [UNK] [UNK] ? \n",
            "answer:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n"
          ]
        }
      ],
      "source": [
        "pred_start, pred_end = pred\n",
        "count = 0\n",
        "eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "for idx, (start, end) in enumerate(zip(pred_start[:5], pred_end[:5])):\n",
        "    squad_eg = eval_examples_no_skip[idx]\n",
        "    offsets = squad_eg.context_token_to_char\n",
        "    start = np.argmax(start)\n",
        "    end = np.argmax(end)\n",
        "    if start >= len(offsets):\n",
        "      continue\n",
        "    pred_char_start = offsets[start][0]\n",
        "    if end < len(offsets):\n",
        "        pred_char_end = offsets[end][1]\n",
        "        pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "    else:\n",
        "        pred_ans = squad_eg.context[pred_char_start:]\n",
        "    cont = tokenizer.detokenize(x_eval[0][idx])\n",
        "    context,question,_ = cont.numpy().decode().split('[SEP]')\n",
        "    print('context: ',context)\n",
        "    print('question: ',question)\n",
        "    print('answer: ',pred_ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References :\n",
        "\n",
        "https://keras.io/examples/nlp/text_extraction_with_bert/"
      ],
      "metadata": {
        "id": "4y_usN-_8ifh"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}